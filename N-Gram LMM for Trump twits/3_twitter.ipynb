{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twit like a Trump \n",
    "Given the Trump Twitter Archive (~290 tweets attributed to the former \n",
    "US president, available among the class materials)\n",
    "- acquire two language models (one bi-gram and one tri-gram) on this set of texts;\n",
    "- use the two models to produce tweets\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "The ng-gram (generic) model can be found under the src folder:\n",
    "- base.py, basic implementation\n",
    "- log.py, log probs instead of using word frequencies as probs\n",
    "- smooth.py, implemented laplace smoothing to handle normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Peter Piper picked a peck of pickled pepper. ', \"Where's the pickled pepper that Peter Piper picked?\"]\n",
      "P(Peter|('<s>',)) = 0.5\n",
      "P(Piper|('Peter',)) = 1.0\n",
      "P(picked|('Piper',)) = 1.0\n",
      "P(</s>|('picked',)) = 0.5\n",
      "Probability of the sentence: 0.25\n",
      "Sentence Probability: 0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\repos\\TLN\\Radicioni\\slides\\labs\\lab 3 twitter\\src\\base.py:25: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  cleaned_words = [re.sub(r\"[^\\w\\s]\", \"\", word)   for word in cleaned_words if re.sub(r\"[^\\w\\s]\", \"\", word) is not '']\n",
      "c:\\Users\\User\\Documents\\repos\\TLN\\Radicioni\\slides\\labs\\lab 3 twitter\\src\\base.py:25: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  cleaned_words = [re.sub(r\"[^\\w\\s]\", \"\", word)   for word in cleaned_words if re.sub(r\"[^\\w\\s]\", \"\", word) is not '']\n",
      "c:\\Users\\User\\Documents\\repos\\TLN\\Radicioni\\slides\\labs\\lab 3 twitter\\src\\base.py:25: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  cleaned_words = [re.sub(r\"[^\\w\\s]\", \"\", word)   for word in cleaned_words if re.sub(r\"[^\\w\\s]\", \"\", word) is not '']\n",
      "c:\\Users\\User\\Documents\\repos\\TLN\\Radicioni\\slides\\labs\\lab 3 twitter\\src\\base.py:25: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  cleaned_words = [re.sub(r\"[^\\w\\s]\", \"\", word)   for word in cleaned_words if re.sub(r\"[^\\w\\s]\", \"\", word) is not '']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mstring\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m NGramLanguageModel \n\u001b[0;32m     16\u001b[0m get_ipython()\u001b[39m.\u001b[39mrun_line_magic(\u001b[39m'\u001b[39m\u001b[39mload_ext\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mautoreload\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     17\u001b[0m get_ipython()\u001b[39m.\u001b[39mrun_line_magic(\u001b[39m'\u001b[39m\u001b[39mreload_ext\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mautoreload\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\repos\\TLN\\Radicioni\\slides\\labs\\lab 3 twitter\\src\\base.py:161\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[39m# Generate new text using the model\u001b[39;00m\n\u001b[0;32m    160\u001b[0m seed \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 161\u001b[0m generated_text \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(max_length\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m)\n\u001b[0;32m    162\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mGenerated Text:\u001b[39m\u001b[39m'\u001b[39m, (generated_text))\n\u001b[0;32m    163\u001b[0m \u001b[39m#model.printDataframe()\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\repos\\TLN\\Radicioni\\slides\\labs\\lab 3 twitter\\src\\base.py:113\u001b[0m, in \u001b[0;36mNGramLanguageModel.generate\u001b[1;34m(self, seed, max_length, top_k)\u001b[0m\n\u001b[0;32m    110\u001b[0m prev_words \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(seed)[\u001b[39m-\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m):]\n\u001b[0;32m    111\u001b[0m sentence \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(seed)\n\u001b[1;32m--> 113\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mlen\u001b[39;49m(sentence) \u001b[39m<\u001b[39m max_length:\n\u001b[0;32m    114\u001b[0m     possible_next_words \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mngrams[\u001b[39mtuple\u001b[39m(prev_words)]\n\u001b[0;32m    115\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m possible_next_words:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Import necessary libraries and modules\n",
    "import random\n",
    "import nltk\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import math \n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "\n",
    "%load_ext autoreload\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base ngram\n",
    "\n",
    "### Defining the ng gram model blue print\n",
    "The provided code snippet represents a training function for an n-gram language model. Let's break down its steps:\n",
    "\n",
    "**Preprocessing**: The function takes a corpus as input, which is a collection of sentences or text. It preprocesses each sentence by tokenizing it into individual words or tokens. The preprocessSentence function is called to perform any necessary preprocessing steps, such as removing punctuation and lowercasing. The processed sentences are then padded with start and end tokens '( <s.> and </s.> )' to delimit the boundaries of each sentence.\n",
    "\n",
    "**Generating n-grams**: The function generates n-grams from the processed corpus. It iterates over the range of the corpus length minus n plus 1. For each iteration, it creates a tuple of the previous n-1 words as the key (prev_words) and the current word as the value (current_word). It updates the ngrams dictionary by incrementing the frequency count of the current word in the context of the previous n-1 words. 'For language modeling, it's better to generate n-grams by considering the word order.'\n",
    "\n",
    "**Probability estimation**: After generating the n-grams, the function proceeds to estimate the probabilities of each word given the previous n-1 words. It iterates over the ngrams dictionary and calculates the total count of next words (total_count) for each context. Then, it computes the probability of each word in the context by dividing its frequency count by the total count. The probabilities are stored in the ngrams dictionary under the 'probability' key."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self,corpus):\n",
    "        # Preprocess corpus to add start and end tokens\n",
    "        # + PADDING of the sentence -> are added two markers (<s> and </s> ) that delimit the sentences\n",
    "        processed_corpus = []\n",
    "        for sentence in corpus:\n",
    "            \n",
    "            tokenized_sentence = self.preprocessSentence(sentence)\n",
    "            # add padding\n",
    "            processed_sentence = ['<s>'] * (self.n - 1) + tokenized_sentence + ['</s>']\n",
    "            processed_corpus.extend(processed_sentence)\n",
    "\n",
    "        # Generate n-grams from the corpus\n",
    "        for i in range(len(processed_corpus) - self.n + 1):\n",
    "            prev_words = tuple(processed_corpus[i:i+self.n-1])\n",
    "            current_word = processed_corpus[i+self.n-1]\n",
    "\n",
    "            self.ngrams[prev_words][current_word]['frequency'] += 1\n",
    "            self.ngrams[prev_words][current_word]['probability'] += 1\n",
    "\n",
    "        # Normalize counts to estimate probabilities using MLE from self.ngrams\n",
    "        for prev_words, next_words in self.ngrams.items():\n",
    "            total_count = sum(next_words[word]['frequency'] for word in next_words)\n",
    "            for word in next_words:\n",
    "                self.ngrams[prev_words][word]['probability'] = next_words[word]['frequency'] / total_count"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the ngram dictionary on this simple example (taken by the slides). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Peter Piper picked a peck of pickled pepper. ', \"Where's the pickled pepper that Peter Piper picked?\"]\n",
      "P(Peter|('<s>',)) = 0.5\n",
      "P(Piper|('Peter',)) = 1.0\n",
      "P(picked|('Piper',)) = 1.0\n",
      "P(</s>|('picked',)) = 0.5\n",
      "Probability of the sentence: 0.25\n",
      "Sentence Probability: 0.25\n",
      "Generated Text: ['<s>', 'Peter', 'Piper', 'picked', 'a', 'peck', 'of', 'pickled', 'pepper', '</s>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\repos\\TLN\\Radicioni\\slides\\labs\\lab 3 twitter\\src\\base.py:25: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  cleaned_words = [re.sub(r\"[^\\w\\s]\", \"\", word)   for word in cleaned_words if re.sub(r\"[^\\w\\s]\", \"\", word) is not '']\n",
      "c:\\Users\\User\\Documents\\repos\\TLN\\Radicioni\\slides\\labs\\lab 3 twitter\\src\\base.py:62: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead.\n",
      "  df = pd.concat([df.drop(['next_words'], axis=1), json_normalize(df['next_words'])], axis=1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prev_words</th>\n",
       "      <th>frequency</th>\n",
       "      <th>Peter.frequency</th>\n",
       "      <th>Peter.probability</th>\n",
       "      <th>Where.frequency</th>\n",
       "      <th>Where.probability</th>\n",
       "      <th>Piper.frequency</th>\n",
       "      <th>Piper.probability</th>\n",
       "      <th>picked.frequency</th>\n",
       "      <th>picked.probability</th>\n",
       "      <th>...</th>\n",
       "      <th>pepper.frequency</th>\n",
       "      <th>pepper.probability</th>\n",
       "      <th>that.frequency</th>\n",
       "      <th>that.probability</th>\n",
       "      <th>&lt;s&gt;.frequency</th>\n",
       "      <th>&lt;s&gt;.probability</th>\n",
       "      <th>s.frequency</th>\n",
       "      <th>s.probability</th>\n",
       "      <th>the.frequency</th>\n",
       "      <th>the.probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(&lt;s&gt;,)</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Peter,)</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Piper,)</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(picked,)</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(a,)</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(peck,)</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(of,)</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(pickled,)</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(pepper,)</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(&lt;/s&gt;,)</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(Where,)</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(s,)</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(the,)</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(that,)</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    prev_words  frequency  Peter.frequency  Peter.probability  \\\n",
       "0       (<s>,)          2              1.0                0.5   \n",
       "1     (Peter,)          1              NaN                NaN   \n",
       "2     (Piper,)          1              NaN                NaN   \n",
       "3    (picked,)          2              NaN                NaN   \n",
       "4         (a,)          1              NaN                NaN   \n",
       "5      (peck,)          1              NaN                NaN   \n",
       "6        (of,)          1              NaN                NaN   \n",
       "7   (pickled,)          1              NaN                NaN   \n",
       "8    (pepper,)          2              NaN                NaN   \n",
       "9      (</s>,)          1              NaN                NaN   \n",
       "10    (Where,)          1              NaN                NaN   \n",
       "11        (s,)          1              NaN                NaN   \n",
       "12      (the,)          1              NaN                NaN   \n",
       "13     (that,)          1              1.0                1.0   \n",
       "\n",
       "    Where.frequency  Where.probability  Piper.frequency  Piper.probability  \\\n",
       "0               1.0                0.5              NaN                NaN   \n",
       "1               NaN                NaN              2.0                1.0   \n",
       "2               NaN                NaN              NaN                NaN   \n",
       "3               NaN                NaN              NaN                NaN   \n",
       "4               NaN                NaN              NaN                NaN   \n",
       "5               NaN                NaN              NaN                NaN   \n",
       "6               NaN                NaN              NaN                NaN   \n",
       "7               NaN                NaN              NaN                NaN   \n",
       "8               NaN                NaN              NaN                NaN   \n",
       "9               NaN                NaN              NaN                NaN   \n",
       "10              NaN                NaN              NaN                NaN   \n",
       "11              NaN                NaN              NaN                NaN   \n",
       "12              NaN                NaN              NaN                NaN   \n",
       "13              NaN                NaN              NaN                NaN   \n",
       "\n",
       "    picked.frequency  picked.probability  ...  pepper.frequency  \\\n",
       "0                NaN                 NaN  ...               NaN   \n",
       "1                NaN                 NaN  ...               NaN   \n",
       "2                2.0                 1.0  ...               NaN   \n",
       "3                NaN                 NaN  ...               NaN   \n",
       "4                NaN                 NaN  ...               NaN   \n",
       "5                NaN                 NaN  ...               NaN   \n",
       "6                NaN                 NaN  ...               NaN   \n",
       "7                NaN                 NaN  ...               2.0   \n",
       "8                NaN                 NaN  ...               NaN   \n",
       "9                NaN                 NaN  ...               NaN   \n",
       "10               NaN                 NaN  ...               NaN   \n",
       "11               NaN                 NaN  ...               NaN   \n",
       "12               NaN                 NaN  ...               NaN   \n",
       "13               NaN                 NaN  ...               NaN   \n",
       "\n",
       "    pepper.probability  that.frequency  that.probability  <s>.frequency  \\\n",
       "0                  NaN             NaN               NaN            NaN   \n",
       "1                  NaN             NaN               NaN            NaN   \n",
       "2                  NaN             NaN               NaN            NaN   \n",
       "3                  NaN             NaN               NaN            NaN   \n",
       "4                  NaN             NaN               NaN            NaN   \n",
       "5                  NaN             NaN               NaN            NaN   \n",
       "6                  NaN             NaN               NaN            NaN   \n",
       "7                  1.0             NaN               NaN            NaN   \n",
       "8                  NaN             1.0               0.5            NaN   \n",
       "9                  NaN             NaN               NaN            1.0   \n",
       "10                 NaN             NaN               NaN            NaN   \n",
       "11                 NaN             NaN               NaN            NaN   \n",
       "12                 NaN             NaN               NaN            NaN   \n",
       "13                 NaN             NaN               NaN            NaN   \n",
       "\n",
       "    <s>.probability  s.frequency  s.probability  the.frequency  \\\n",
       "0               NaN          NaN            NaN            NaN   \n",
       "1               NaN          NaN            NaN            NaN   \n",
       "2               NaN          NaN            NaN            NaN   \n",
       "3               NaN          NaN            NaN            NaN   \n",
       "4               NaN          NaN            NaN            NaN   \n",
       "5               NaN          NaN            NaN            NaN   \n",
       "6               NaN          NaN            NaN            NaN   \n",
       "7               NaN          NaN            NaN            NaN   \n",
       "8               NaN          NaN            NaN            NaN   \n",
       "9               1.0          NaN            NaN            NaN   \n",
       "10              NaN          1.0            1.0            NaN   \n",
       "11              NaN          NaN            NaN            1.0   \n",
       "12              NaN          NaN            NaN            NaN   \n",
       "13              NaN          NaN            NaN            NaN   \n",
       "\n",
       "    the.probability  \n",
       "0               NaN  \n",
       "1               NaN  \n",
       "2               NaN  \n",
       "3               NaN  \n",
       "4               NaN  \n",
       "5               NaN  \n",
       "6               NaN  \n",
       "7               NaN  \n",
       "8               NaN  \n",
       "9               NaN  \n",
       "10              NaN  \n",
       "11              1.0  \n",
       "12              NaN  \n",
       "13              NaN  \n",
       "\n",
       "[14 rows x 30 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\repos\\TLN\\Radicioni\\slides\\labs\\lab 3 twitter\\src\\base.py:62: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead.\n",
      "  df = pd.concat([df.drop(['next_words'], axis=1), json_normalize(df['next_words'])], axis=1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prev_words</th>\n",
       "      <th>frequency</th>\n",
       "      <th>Peter.frequency</th>\n",
       "      <th>Peter.probability</th>\n",
       "      <th>Where.frequency</th>\n",
       "      <th>Where.probability</th>\n",
       "      <th>Piper.frequency</th>\n",
       "      <th>Piper.probability</th>\n",
       "      <th>picked.frequency</th>\n",
       "      <th>picked.probability</th>\n",
       "      <th>...</th>\n",
       "      <th>pepper.frequency</th>\n",
       "      <th>pepper.probability</th>\n",
       "      <th>that.frequency</th>\n",
       "      <th>that.probability</th>\n",
       "      <th>&lt;s&gt;.frequency</th>\n",
       "      <th>&lt;s&gt;.probability</th>\n",
       "      <th>s.frequency</th>\n",
       "      <th>s.probability</th>\n",
       "      <th>the.frequency</th>\n",
       "      <th>the.probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(&lt;s&gt;, &lt;s&gt;)</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(&lt;s&gt;, Peter)</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Peter, Piper)</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Piper, picked)</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(picked, a)</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(a, peck)</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(peck, of)</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(of, pickled)</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(pickled, pepper)</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(pepper, &lt;/s&gt;)</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(&lt;/s&gt;, &lt;s&gt;)</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(&lt;s&gt;, Where)</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(Where, s)</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(s, the)</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(the, pickled)</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(pepper, that)</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(that, Peter)</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           prev_words  frequency  Peter.frequency  Peter.probability  \\\n",
       "0          (<s>, <s>)          2              1.0                0.5   \n",
       "1        (<s>, Peter)          1              NaN                NaN   \n",
       "2      (Peter, Piper)          1              NaN                NaN   \n",
       "3     (Piper, picked)          2              NaN                NaN   \n",
       "4         (picked, a)          1              NaN                NaN   \n",
       "5           (a, peck)          1              NaN                NaN   \n",
       "6          (peck, of)          1              NaN                NaN   \n",
       "7       (of, pickled)          1              NaN                NaN   \n",
       "8   (pickled, pepper)          2              NaN                NaN   \n",
       "9      (pepper, </s>)          1              NaN                NaN   \n",
       "10        (</s>, <s>)          1              NaN                NaN   \n",
       "11       (<s>, Where)          1              NaN                NaN   \n",
       "12         (Where, s)          1              NaN                NaN   \n",
       "13           (s, the)          1              NaN                NaN   \n",
       "14     (the, pickled)          1              NaN                NaN   \n",
       "15     (pepper, that)          1              1.0                1.0   \n",
       "16      (that, Peter)          1              NaN                NaN   \n",
       "\n",
       "    Where.frequency  Where.probability  Piper.frequency  Piper.probability  \\\n",
       "0               1.0                0.5              NaN                NaN   \n",
       "1               NaN                NaN              1.0                1.0   \n",
       "2               NaN                NaN              NaN                NaN   \n",
       "3               NaN                NaN              NaN                NaN   \n",
       "4               NaN                NaN              NaN                NaN   \n",
       "5               NaN                NaN              NaN                NaN   \n",
       "6               NaN                NaN              NaN                NaN   \n",
       "7               NaN                NaN              NaN                NaN   \n",
       "8               NaN                NaN              NaN                NaN   \n",
       "9               NaN                NaN              NaN                NaN   \n",
       "10              NaN                NaN              NaN                NaN   \n",
       "11              NaN                NaN              NaN                NaN   \n",
       "12              NaN                NaN              NaN                NaN   \n",
       "13              NaN                NaN              NaN                NaN   \n",
       "14              NaN                NaN              NaN                NaN   \n",
       "15              NaN                NaN              NaN                NaN   \n",
       "16              NaN                NaN              1.0                1.0   \n",
       "\n",
       "    picked.frequency  picked.probability  ...  pepper.frequency  \\\n",
       "0                NaN                 NaN  ...               NaN   \n",
       "1                NaN                 NaN  ...               NaN   \n",
       "2                2.0                 1.0  ...               NaN   \n",
       "3                NaN                 NaN  ...               NaN   \n",
       "4                NaN                 NaN  ...               NaN   \n",
       "5                NaN                 NaN  ...               NaN   \n",
       "6                NaN                 NaN  ...               NaN   \n",
       "7                NaN                 NaN  ...               1.0   \n",
       "8                NaN                 NaN  ...               NaN   \n",
       "9                NaN                 NaN  ...               NaN   \n",
       "10               NaN                 NaN  ...               NaN   \n",
       "11               NaN                 NaN  ...               NaN   \n",
       "12               NaN                 NaN  ...               NaN   \n",
       "13               NaN                 NaN  ...               NaN   \n",
       "14               NaN                 NaN  ...               1.0   \n",
       "15               NaN                 NaN  ...               NaN   \n",
       "16               NaN                 NaN  ...               NaN   \n",
       "\n",
       "    pepper.probability  that.frequency  that.probability  <s>.frequency  \\\n",
       "0                  NaN             NaN               NaN            NaN   \n",
       "1                  NaN             NaN               NaN            NaN   \n",
       "2                  NaN             NaN               NaN            NaN   \n",
       "3                  NaN             NaN               NaN            NaN   \n",
       "4                  NaN             NaN               NaN            NaN   \n",
       "5                  NaN             NaN               NaN            NaN   \n",
       "6                  NaN             NaN               NaN            NaN   \n",
       "7                  1.0             NaN               NaN            NaN   \n",
       "8                  NaN             1.0               0.5            NaN   \n",
       "9                  NaN             NaN               NaN            1.0   \n",
       "10                 NaN             NaN               NaN            1.0   \n",
       "11                 NaN             NaN               NaN            NaN   \n",
       "12                 NaN             NaN               NaN            NaN   \n",
       "13                 NaN             NaN               NaN            NaN   \n",
       "14                 1.0             NaN               NaN            NaN   \n",
       "15                 NaN             NaN               NaN            NaN   \n",
       "16                 NaN             NaN               NaN            NaN   \n",
       "\n",
       "    <s>.probability  s.frequency  s.probability  the.frequency  \\\n",
       "0               NaN          NaN            NaN            NaN   \n",
       "1               NaN          NaN            NaN            NaN   \n",
       "2               NaN          NaN            NaN            NaN   \n",
       "3               NaN          NaN            NaN            NaN   \n",
       "4               NaN          NaN            NaN            NaN   \n",
       "5               NaN          NaN            NaN            NaN   \n",
       "6               NaN          NaN            NaN            NaN   \n",
       "7               NaN          NaN            NaN            NaN   \n",
       "8               NaN          NaN            NaN            NaN   \n",
       "9               1.0          NaN            NaN            NaN   \n",
       "10              1.0          NaN            NaN            NaN   \n",
       "11              NaN          1.0            1.0            NaN   \n",
       "12              NaN          NaN            NaN            1.0   \n",
       "13              NaN          NaN            NaN            NaN   \n",
       "14              NaN          NaN            NaN            NaN   \n",
       "15              NaN          NaN            NaN            NaN   \n",
       "16              NaN          NaN            NaN            NaN   \n",
       "\n",
       "    the.probability  \n",
       "0               NaN  \n",
       "1               NaN  \n",
       "2               NaN  \n",
       "3               NaN  \n",
       "4               NaN  \n",
       "5               NaN  \n",
       "6               NaN  \n",
       "7               NaN  \n",
       "8               NaN  \n",
       "9               NaN  \n",
       "10              NaN  \n",
       "11              NaN  \n",
       "12              1.0  \n",
       "13              NaN  \n",
       "14              NaN  \n",
       "15              NaN  \n",
       "16              NaN  \n",
       "\n",
       "[17 rows x 30 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.base import NGramLanguageModel \n",
    "\n",
    "# Create an instance of the n-gram language model with n=2 (bigram model)\n",
    "bi_model = NGramLanguageModel(2)\n",
    "tri_model = NGramLanguageModel(3)\n",
    "\n",
    "# Train the model on a corpus (a list of sentences or words)\n",
    "corpus =[\n",
    "    'Peter Piper picked a peck of pickled pepper. ',\n",
    "    \"Where's the pickled pepper that Peter Piper picked?\",\n",
    "]\n",
    "\n",
    "bi_model.train(corpus)\n",
    "bi_model.printDataframe()\n",
    "\n",
    "tri_model.train(corpus)\n",
    "tri_model.printDataframe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of sentence_probability \n",
    "self.ngrams[prev_words] contains the prefix (n-1)  of the n-gram, while self.ngrams[prev_words] [next_word] contains the possible suffixes of the current n-gram.\n",
    "NGramLanguageModel is the basic implementation of the n gram language model that doesn't use log probabiliies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_probability(self, sentence,debug=False):\n",
    "        # Preprocess the input sentence\n",
    "        tokenized_sentence = self.preprocessSentence(sentence)\n",
    "        processed_sentence = ['<s>'] * (self.n - 1) + tokenized_sentence + ['</s>']\n",
    "\n",
    "        # Initialize probability to maximum in logaritmic space\n",
    "        probability = 1.0\n",
    "        \n",
    "        \n",
    "        # Iterate over the sentence to compute the probability\n",
    "        for i in range(len(processed_sentence) - self.n + 1):\n",
    "            prev_words = tuple(processed_sentence[i:i+self.n-1])\n",
    "            current_word = processed_sentence[i+self.n-1]\n",
    "\n",
    "            # Check if the n-gram exists in the language model\n",
    "            if prev_words in self.ngrams and current_word in self.ngrams[prev_words]:\n",
    "                # Multiply the probability by the conditional probability of the current word given the previous words\n",
    "                prob = self.ngrams[prev_words][current_word]['probability']\n",
    "                log_prob = np.log(prob)\n",
    "                probability *= prob\n",
    "                if debug:\n",
    "                    print(\"P({}|{}) = {}\".format(current_word, prev_words, prob))\n",
    "            else:\n",
    "                # if the n-gram doesn't exist, return 0.0 or smooth the probability\n",
    "                probability *=0.0000000001\n",
    "                if debug:\n",
    "                    print(\"P({}|{}) = {}\".format(current_word, prev_words, 'N/A'))\n",
    "\n",
    "        if(debug):\n",
    "            print(\"Probability of the sentence: {}\".format(probability))\n",
    "        return probability"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let us compute P(Peter Piper picked)** \n",
    "For the sake of this example, I have removed the punctuation in the pre processing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram model\n",
      "P(Peter|('<s>',)) = 0.5\n",
      "P(Piper|('Peter',)) = 1.0\n",
      "P(picked|('Piper',)) = 1.0\n",
      "P(</s>|('picked',)) = 0.5\n",
      "Probability of the sentence: 0.25\n",
      "\n",
      "\n",
      "Trigram model\n",
      "P(Peter|('<s>', '<s>')) = 0.5\n",
      "P(Piper|('<s>', 'Peter')) = 1.0\n",
      "P(picked|('Peter', 'Piper')) = 1.0\n",
      "P(</s>|('Piper', 'picked')) = 0.5\n",
      "Probability of the sentence: 0.25\n"
     ]
    }
   ],
   "source": [
    "# Train the model on a corpus (a list of sentences or words)\n",
    "corpus =[\n",
    "    'Peter Piper picked a peck of pickled pepper. ',\n",
    "    \"Where's the pickled pepper that Peter Piper picked?\",\n",
    "]\n",
    "\n",
    "sentence = \"Peter Piper picked\"\n",
    "\n",
    "print(\"Bigram model\")\n",
    "bi_probability = bi_model.sentence_probability(sentence,debug=True)\n",
    "print('\\n')\n",
    "print(\"Trigram model\")\n",
    "tri_probability = tri_model.sentence_probability(sentence,debug=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the treatment of punctuation in an n-gram language model depends on the specific use case and requirements. Including I include punctuation in the training data, the language model will treat it as part of the n-gram context. This means that n-grams will be formed including punctuation, and the model will learn patterns that involve punctuation marks. This can be useful if you want the model to capture punctuation-related information, such as sentence boundaries or specific punctuation usage patterns."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now test the generative capabilities of the model:\n",
    "- seed, is a list of words that guide the text generation ( from which the consecutive ngram are chosen)\n",
    "- max lenght, max number of words of the generated textù\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(self, seed=None, max_length=10,top_k=5):\n",
    "        if seed is None:\n",
    "            seed = ['<s>'] * (self.n - 1)\n",
    "        \n",
    "        prev_words = tuple(seed)[-(self.n - 1):]\n",
    "        sentence = list(seed)\n",
    "        \n",
    "        while len(sentence) < max_length:\n",
    "            possible_next_words = self.ngrams[tuple(prev_words)]\n",
    "            if not possible_next_words:\n",
    "                break\n",
    "            \n",
    "            # Select the top-k most probable words\n",
    "            top_words = sorted(possible_next_words.keys(),\n",
    "                           key=lambda word: possible_next_words[word]['probability'],\n",
    "                           reverse=True)[:top_k]\n",
    "\n",
    "            next_word = random.choice(list(top_words))\n",
    "            # append the selected word to the sentence extract the word from the list\n",
    "            sentence.append(next_word)\n",
    "            \n",
    "            # update the previous words for the next iteration\n",
    "            # remove the first word and add the selected word at the end\n",
    "            prev_words = prev_words[1:] + (next_word,)\n",
    "                \n",
    "        \n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram model\n",
      "Generated Text: <s> Peter Piper picked a peck of pickled pepper </s>\n",
      "Generated Text: a peck of pickled pepper that Peter\n",
      "Generated Text: a peck of\n",
      "Generated Text: peck a\n",
      "Trigram model\n",
      "Generated Text: <s> <s> Peter Piper picked a peck of pickled pepper that\n",
      "Generated Text: a peck of pickled pepper </s> <s>\n",
      "Generated Text: a peck of\n"
     ]
    }
   ],
   "source": [
    "print(\"Bigram model\")\n",
    "\n",
    "seed = None\n",
    "generated_text = bi_model.generate(max_length=20)\n",
    "print('Generated Text:', (\" \".join(generated_text)))\n",
    "# Generate new text using the model\n",
    "seed = ['a', 'peck']\n",
    "generated_text = bi_model.generate(seed=seed,max_length=7)\n",
    "print('Generated Text:', (\" \".join(generated_text)))\n",
    "# Generate new text using the model with top-k = 1, so that the most probable word is always selected\n",
    "seed = ['a', 'peck']\n",
    "generated_text = bi_model.generate(seed=seed,max_length=3)\n",
    "print('Generated Text:', (\" \".join(generated_text)))\n",
    "# Generate new text using the model with top-k = 1, so that the most probable word is always selected\n",
    "seed = ['peck','a']\n",
    "generated_text = bi_model.generate(seed=seed,max_length=3,top_k=1)\n",
    "print('Generated Text:', (\" \".join(generated_text)))\n",
    "\n",
    "\n",
    "print(\"Trigram model\")\n",
    "seed = None\n",
    "generated_text = tri_model.generate(max_length=20)\n",
    "print('Generated Text:', (\" \".join(generated_text)))\n",
    "# Generate new text using the model\n",
    "seed = ['a', 'peck']\n",
    "generated_text = tri_model.generate(seed=seed,max_length=7)\n",
    "print('Generated Text:', (\" \".join(generated_text)))\n",
    "# Generate new text using the model with top-k = 1, so that the most probable word is always selected\n",
    "seed = ['a', 'peck']\n",
    "generated_text = tri_model.generate(seed=seed,max_length=3,top_k=1)\n",
    "print('Generated Text:', (\" \".join(generated_text)))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log NGram\n",
    "The only difference is the usage of log probabilities (instead of multiplying probs, now we are adding them)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smooth NGram\n",
    "- sentence pre processing, words not found in the unique token are converted to unk\n",
    " tokens = [token if token in self.ngrams.keys()  else '<unk>' for token in tokens]\n",
    "- applied laplace smoothing in the training algo\n",
    "\n",
    "\n",
    "          #add the <unk> token to the vocabulary and dont initialize it\n",
    "        self.ngrams[('<unk>',)]= {}\n",
    "        # Apply Laplace smoothing and normalize counts to estimate probabilities\n",
    "        vocabulary_size = len (self.ngrams.keys())  # Size of the vocabulary, included the padding\n",
    "        for prev_words, next_words in self.ngrams.items():\n",
    "            total_count = sum(next_words[word]['frequency'] for word in next_words)\n",
    "            for word in next_words:\n",
    "                word_count = next_words[word]['frequency']\n",
    "                smoothed_count = word_count + 1  # Apply Laplace smoothing\n",
    "                smoothed_probability = smoothed_count / (total_count + vocabulary_size)\n",
    "                self.ngrams[prev_words][word]['probability'] = smoothed_probability\n",
    "                self.ngrams[prev_words][word]['frequency'] += 1\n",
    "\n",
    "    \n",
    "            remaining_words = set(self.word_tokens) - set(next_words.keys()) \n",
    "            for word in remaining_words:\n",
    "                smoothed_probability = 1 / (total_count + vocabulary_size)\n",
    "                if(self.ngrams[prev_words].get(word) is None):  \n",
    "                    self.ngrams[prev_words][word] = defaultdict(lambda: {\"frequency\": 0, \"probability\": 0.0})\n",
    "               \n",
    "                self.ngrams[prev_words][word]['frequency'] =1 \n",
    "                self.ngrams[prev_words][word]['probability'] =smoothed_probability\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on the Twitter dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(filename):\n",
    "    dataset = []\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        # Create a CSV reader object\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            if not row[1].startswith(\"@\"):\n",
    "                dataset.append((row[1]))  \n",
    "    return dataset\n",
    "\n",
    "file_path = 'data/tweets.csv'\n",
    "df = read_csv(file_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the dataset into train and test sets\n",
    "train_size = int(len(df) * 0.8)\n",
    "train_set = df[:train_size]\n",
    "test_set = df[train_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Peter Piper picked a peck of pickled pepper. ', \"Where's the pickled pepper that Peter Piper picked?\"]\n",
      "Sentence Probability: -2.0794415416798357\n",
      "Sentence Probability: -69.77069997038132\n",
      "Input 1 Sentence Perplexity: 1.2968395546510096\n",
      "Input 2 Sentence Perplexity: 37606030.93086393\n",
      "Generated Text: ['<s>', 'Where', 's', 'the', 'pickled', 'pepper', 'that', 'Peter', 'Piper', 'picked', '</s>']\n",
      "['Peter Piper picked a peck of pickled pepper. ', \"Where's the pickled pepper that Peter Piper picked?\"]\n",
      "Sentence Probability: -23.294367526066186\n",
      "Sentence Probability: -161.18095650958318\n",
      "Input 1 Sentence Perplexity: 13.306638665166679\n",
      "Input 2 Sentence Perplexity: 464158883361.2762\n",
      "Generated Text: ['<s>', '<s>', 'Piper']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from src.log import NGramLanguageModelLogProbs \n",
    "from src.smooth import NGramLanguageModelSmoothing \n",
    "\n",
    "models_names = ['NGramLanguageModel','NGramLanguageModelLogProbs','NGramLanguageModelSmoothing']\n",
    "\n",
    "bi_models = {\"NGramLanguageModel\":NGramLanguageModel(2),\"NGramLanguageModelLogProbs\":NGramLanguageModelLogProbs(2),\"NGramLanguageModelSmoothing\":NGramLanguageModelSmoothing(2)}\n",
    "tri_models = {\"NGramLanguageModel\":NGramLanguageModel(3),\"NGramLanguageModelLogProbs\":NGramLanguageModelLogProbs(3),\"NGramLanguageModelSmoothing\":NGramLanguageModelSmoothing(3)}\n",
    "\n",
    "for bi_model in bi_models.values():\n",
    "    bi_model.train(train_set) \n",
    "\n",
    "for tri_model in tri_models.values():\n",
    "    tri_model.train(train_set) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate all of the models on the test set and print the perplexity scores\n",
    "bi_models_perplexities =  {'NGramLanguageModelLogProbs':[],'NGramLanguageModelSmoothing':[]}\n",
    "tri_models_perplexities = {'NGramLanguageModelLogProbs':[],'NGramLanguageModelSmoothing':[]}\n",
    "\n",
    "for model in models_names[1:]:\n",
    "    \n",
    "    for sentence in test_set:\n",
    "        bi_models_perplexities[model].append(bi_models[model].perplexity(sentence))\n",
    "        tri_models_perplexities[model].append(tri_models[model].perplexity(sentence))\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BI-GRAM Models: counting the number of times the Smoothing model has a lower perplexity score than LogProbs\n",
      "\n",
      "NGramLanguageModelSmoothing    33\n",
      "NGramLanguageModelLogProbs      2\n",
      "Name: better_model (lower ppl), dtype: int64\n",
      "\n",
      "\n",
      "TRI-GRAM Models: counting the number of times the Smoothing model has a lower perplexity score than LogProbs\n",
      "\n",
      "NGramLanguageModelSmoothing    33\n",
      "NGramLanguageModelLogProbs      2\n",
      "Name: better_model(lower ppl), dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#compare the perplexity scores of the models bi-gram vs tri-gram\n",
    "bi_models_perplexities_df = pd.DataFrame.from_dict(bi_models_perplexities)\n",
    "tri_models_perplexities_df = pd.DataFrame.from_dict(tri_models_perplexities)\n",
    "\n",
    "bi_models_perplexities_df['better_model (lower ppl)'] = bi_models_perplexities_df.idxmin(axis=1)\n",
    "tri_models_perplexities_df['better_model(lower ppl)'] = tri_models_perplexities_df.idxmin(axis=1)\n",
    "\n",
    "print(\"BI-GRAM Models: counting the number of times the Smoothing model has a lower perplexity score than LogProbs\\n\")\n",
    "print(bi_models_perplexities_df['better_model (lower ppl)'].value_counts())\n",
    "print('\\n')\n",
    "print(\"TRI-GRAM Models: counting the number of times the Smoothing model has a lower perplexity score than LogProbs\\n\")\n",
    "print(tri_models_perplexities_df['better_model(lower ppl)'].value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally the model that uses SMoothing had better perplexities results, which means that it handles better unknown data.\n",
    "( So the perplexity computed on each sentence of the test set, resulted lower on the Smoothing model-> better)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now see what can be generated by the 2 smoothing models (bi-gram and tri-gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '<s>', 'I', 'have','panelists']\n",
      "ignored by winners.  -- @ CoachJoeGibbs\n"
     ]
    }
   ],
   "source": [
    "print(bi_models['NGramLanguageModelSmoothing'].generate())\n",
    "\n",
    "print(''.join(tri_models['NGramLanguageModelSmoothing'].generate()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '<s>', 'I', 'have', 'not', 'panelists']\n",
      "Obama is laughing at Karl Rove losers-true ! I never said anything bad about"
     ]
    }
   ],
   "source": [
    "\n",
    "print((bi_models['NGramLanguageModelSmoothing'].generate()))\n",
    "print(''.join(tri_models['NGramLanguageModelSmoothing'].generate()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judges Taxes Regulations Healthcare the Military Vets ( Choice ! )\n"
     ]
    }
   ],
   "source": [
    "print(''.join(bi_models['NGramLanguageModelSmoothing'].generate(['Judges'])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With an example seed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
