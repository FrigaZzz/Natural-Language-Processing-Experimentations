{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word sense disambiguation (WSD)\n",
    "It is an open problem of  natural language processing, which comprises the process of identifying which sense of a word (i.e. meaning) is used in any given sentence, when the word has a number of distinct senses (polysemy).\n",
    "\n",
    "This part of the lab requires to implement the Lesk algorithm and evaluate the accuracy of the system.\n",
    "\n",
    "**Type of features**: bag-of-words features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package semcor to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package semcor is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import csv\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus.reader.wordnet import Synset\n",
    "nltk.download('semcor')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words are commonly used words in a language that are often considered insignificant for text analysis and information retrieval tasks.\n",
    "\n",
    "Examples of stop words in English include \"the,\" \"is,\" \"and,\" \"of,\" and so on. These words typically have little or no semantic meaning and are frequently used in the language.\n",
    "\n",
    "I am going to consider the implication of removing/keeping function words (e.i. stopwords ) when applying the Lesk Algorithm to the WSD Task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "671 ['wheres', 'stop', 'less', 'wants', 'whim', 'enough', 'they', 'sufficiently', 'www', 'p']\n"
     ]
    }
   ],
   "source": [
    "stopwords_file = open(f\"./utils/stop_words_FULL.txt\", \"r\")\n",
    "function_words_list = []\n",
    "for word in stopwords_file:\n",
    "    function_words_list.append(word.replace('\\n', ''))\n",
    "stopwords_file.close()\n",
    "\n",
    "# Remove duplicates if any\n",
    "function_words = list(set(function_words_list))\n",
    "print(len(function_words), function_words[:10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define some utils function for handling text pre processing such as:\n",
    "- punctuation removal\n",
    "- function words removal ( which are a super set of the stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog.', 'true?']\n",
      "the quick brown fox jumps over the lazy dog is it true\n"
     ]
    }
   ],
   "source": [
    "def remove_punctuation(text):\n",
    "    pattern = r\"[^\\w\\s]\"  # Matches any character that is not a word character or whitespace\n",
    "    cleaned_text = re.sub(pattern, \"\", text)\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "def remove_stopwords(sentence: str):\n",
    "    words = sentence.split()\n",
    "    cleaned_words = []\n",
    "    for word in words:\n",
    "        if word not in function_words:\n",
    "            cleaned_words.append(word)\n",
    "    return cleaned_words\n",
    "\n",
    "example_sentence = \"The quick brown fox jumps over the lazy dog. Is it true?\".lower()\n",
    "print(remove_stopwords(example_sentence))\n",
    "\n",
    "print(remove_punctuation(example_sentence))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of Word Sense Disambiguation (WSD), the Bag-of-Words (BoW) approach is a common representation technique used to transform textual data into a numerical format that can be processed by machine learning algorithms.\n",
    "\n",
    "The BoW approach treats each document or sentence as an unordered set of words, disregarding the grammar and word order, and focuses solely on the frequency or presence of words. \n",
    "\n",
    "The following implementation considers the simplest bow rappresentation:  the context of a target word is rappresented by a vector of features, each binary feature indicating whether a vocabulary word w does or doesnâ€™t occur in the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog', 'true']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def build_bag_of_words(phrase: str, cleanse=True) -> list:\n",
    "    \"\"\"\n",
    "    Returns the set of words of a phrase.\n",
    "    \"\"\"\n",
    "    sentence = phrase.lower()\n",
    "    words = []\n",
    "    sentence: str = remove_punctuation(sentence)\n",
    "    if cleanse:\n",
    "        words = remove_stopwords(sentence)\n",
    "    else:\n",
    "        words = sentence.split()\n",
    "    return words\n",
    "\n",
    "example_sentence = \"The quick brown fox jumps over the lazy dog. Is it true?\"\n",
    "\n",
    "print(build_bag_of_words(example_sentence,cleanse=True))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Lesk algorithm follows these main steps:\n",
    "\n",
    "- Retrieve the context: Gather the surrounding words or the context in which the target word appears. Typically, a window of neighboring words is considered.\n",
    "\n",
    "- Obtain the senses: Retrieve the different senses of the target word from a sense inventory or a lexical resource such as WordNet. Each sense corresponds to a different meaning or sense of the word.\n",
    "\n",
    "- Extract glosses: Retrieve the gloss (definition) for each sense of the target word. The gloss provides a concise description of the meaning of the sense.\n",
    "\n",
    "- Calculate overlap: Compare the words in the context with the words in each sense's gloss. Calculate the overlap or similarity between the context and the gloss by measuring the shared words or their relatedness.\n",
    "\n",
    "- Select the best sense: Determine the sense with the highest overlap or similarity score as the predicted sense for the target word. The assumption is that the sense with the most related words in common with the context is the most likely sense.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_overlap(signature: list, context: list) -> int:\n",
    "    \"\"\"\n",
    "    Returns the number of words in common between signature and context\n",
    "    \"\"\"\n",
    "    total_common_words = 0\n",
    "    unique_words_signature = (set(signature))\n",
    "    unique_words_context = (set(context))\n",
    "    # intersaction between the two sets\n",
    "    common_words = unique_words_signature.intersection(unique_words_context)\n",
    "    total_common_words = len(common_words)\n",
    "    return total_common_words,common_words\n",
    "\n",
    "\n",
    "def get_signature(synset: Synset,cleanse) -> list:\n",
    "    \"\"\"\n",
    "    Returns the signature of synset -> set of words in the gloss and examples of sens\n",
    "    \"\"\"\n",
    "    examples = synset.examples() #examples of given synset\n",
    "    glossary = synset.definition() #glossary of given synset\n",
    "\n",
    "    # given examples list and the glossary pharse, we build the signature\n",
    "    phrases = []\n",
    "    phrases.append(glossary)\n",
    "    phrases.extend(examples)\n",
    "    # each phrase is converted in a bag of words\n",
    "    signature = []\n",
    "    for phrase in phrases:\n",
    "        set_of_words = build_bag_of_words(phrase,cleanse)\n",
    "        signature.extend(set_of_words)\n",
    "    # remove duplicates\n",
    "    signature = list(set(signature))\n",
    "    return signature\n",
    "\n",
    "\n",
    "def lesk(word: str, sentence: str, cleanse = True) -> Synset:\n",
    "    \"\"\"\n",
    "    Returns the best sense of given word used in sentence.\n",
    "\n",
    "    Args\n",
    "        word: word to disambiguate\n",
    "        sentence: sentence where word appears\n",
    "\n",
    "    Returns\n",
    "        best_sense: best sense of word used in sentence\n",
    "    \"\"\" \n",
    "    signature_best_sense = None # signature of best sense of word used in sentence\n",
    "    common_words_best_sense = None # common words between signature and context of best sense of word used in sentence\n",
    "\n",
    "    best_sense = None # best sense of word used in sentence\n",
    "    \n",
    "    max_overlap = 0  # max overlap between signature and context\n",
    "   \n",
    "    # the context is rappresented by the bag of words of the sentence\n",
    "    context = build_bag_of_words(sentence,cleanse) \n",
    "    # for each sense of the word, we compute the overlap between the signature and the context\n",
    "    for synset in wn.synsets(word):\n",
    "        signature = get_signature(synset,cleanse) \n",
    "        overlap,common_words = compute_overlap(signature, context)\n",
    "        # print(synset,signature,context,overlap,common_words)\n",
    "\n",
    "        if overlap > max_overlap:\n",
    "            max_overlap = overlap\n",
    "            best_sense = synset\n",
    "            signature_best_sense = signature\n",
    "            common_words_best_sense = common_words\n",
    "    if(max_overlap> 0 ):\n",
    "        return best_sense, max_overlap, signature_best_sense,common_words_best_sense\n",
    "    return None,None,None,None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('depository_financial_institution.n.01')\n",
      "Synset('depository_financial_institution.n.01') \n",
      " a financial institution that accepts deposits and channels the money into lending activities \n",
      " ['he cashed a check at the bank', 'that bank holds the mortgage on my home'] \n",
      " 3 \n",
      " ['holds', 'accepts', 'channels', 'cashed', 'check', 'financial', 'money', 'activities', 'lending', 'deposits', 'bank', 'mortgage', 'institution'] \n",
      " {'mortgage', 'bank', 'deposits'}\n"
     ]
    }
   ],
   "source": [
    "# test the lesk algorithm\n",
    "sentence = '''the bank can guarantee deposits will eventually cover future tuition\n",
    "costs because it invests in adjustable-rate mortgage securities.'''\n",
    "word = \"bank\"\n",
    "best_sense, max_overlap, signature_best_sense, common_words_best_sense = lesk(word, sentence, cleanse=True)\n",
    "print(best_sense)\n",
    "if(not best_sense):\n",
    "    print(\"No sense found for the word: \",word)\n",
    "else:\n",
    "     print( best_sense,\n",
    "      '\\n', best_sense.definition(),\n",
    "      '\\n', best_sense.examples(),\n",
    "      '\\n', max_overlap,\n",
    "      '\\n', signature_best_sense,\n",
    "      '\\n', common_words_best_sense)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('depository_financial_institution.n.01') \n",
      " a financial institution that accepts deposits and channels the money into lending activities \n",
      " ['he cashed a check at the bank', 'that bank holds the mortgage on my home'] \n",
      " 4 \n",
      " ['home', 'accepts', 'on', 'financial', 'cashed', 'bank', 'institution', 'he', 'holds', 'that', 'at', 'activities', 'the', 'my', 'mortgage', 'into', 'a', 'and', 'lending', 'channels', 'check', 'money', 'deposits'] \n",
      " {'mortgage', 'bank', 'the', 'deposits'}\n"
     ]
    }
   ],
   "source": [
    "# test the lesk algorithm\n",
    "sentence = '''the bank can guarantee deposits will eventually cover future tuition\n",
    "costs because it invests in adjustable-rate mortgage securities. '''\n",
    "word = \"bank\"\n",
    "best_sense, max_overlap, signature_best_sense, common_words_best_sense = lesk(word, sentence,cleanse=False)\n",
    "if(not best_sense):\n",
    "    print(\"No sense found for the word: \",word)\n",
    "else:\n",
    "     print( best_sense,\n",
    "      '\\n', best_sense.definition(),\n",
    "      '\\n', best_sense.examples(),\n",
    "      '\\n', max_overlap,\n",
    "      '\\n', signature_best_sense,\n",
    "      '\\n', common_words_best_sense)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the best sense is 'depository_financial_institution.m.01' but the fact that we are considering function words could easily lead to wrong interpretations. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now test the algorithm on the SemCor Corpus:\n",
    "- Extract 50 sentences from the SemCor corpus (corpus annotated with the synsets of \n",
    "WN) and disambiguate (at least) one noun per sentence. Calculate \n",
    "the accuracy of the implemented system based on the senses annotated in \n",
    "SemCor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', 'Atlanta', \"'s\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import semcor\n",
    "\n",
    "# extract 1 sentence from semcor corpus\n",
    "semcor_sents = semcor.sents()\n",
    "semcor_sent = semcor_sents[0]\n",
    "print(semcor_sent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_sentences_from_semcor(randomExtract=False):\n",
    "    sentences = []\n",
    "    all_sentences = semcor.sents()\n",
    "\n",
    "    if randomExtract:\n",
    "        selected_sentences = random.sample(list(all_sentences[:1000]), 50)\n",
    "    else:\n",
    "        selected_sentences = all_sentences[:50]\n",
    "\n",
    "    for i, sentence in enumerate(selected_sentences):\n",
    "        tagged_sentence = semcor.tagged_sents(tag='sem')[i]\n",
    "        sentences.append((sentence, tagged_sentence))\n",
    "\n",
    "    return sentences\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_nouns_from_sentence(sentenceData, randomExtract=False):\n",
    "    nouns = []\n",
    "    \n",
    "    for targetSense in sentenceData[1]:\n",
    "        if isinstance(targetSense, nltk.tree.Tree):\n",
    "            lemma = targetSense.label()\n",
    "            if lemma:\n",
    "                synset = None\n",
    "                if isinstance(lemma, str):\n",
    "                    synset = wn.synsets(lemma)\n",
    "                else:\n",
    "                    synset = lemma.synset()\n",
    "                if synset and isinstance(synset, Synset) and synset.pos() == 'n':\n",
    "                    for word in targetSense.leaves():\n",
    "                            nouns.append((word, synset))\n",
    "    if(randomExtract):\n",
    "        max_no_of_nouns = random.randint(1, len(nouns))\n",
    "        nouns = random.sample(nouns, max_no_of_nouns)\n",
    "\n",
    "    return nouns\n",
    "\n",
    "\n",
    "\n",
    "# disambiuate the nouns in the sentence\n",
    "def disambiguate_nouns(sentenceData,nounsData,cleanse=False,debug=False):\n",
    "    best_senses =[]\n",
    "    for nounData in nounsData:\n",
    "        noun = nounData[0]\n",
    "        sentence_str = ' '.join(sentenceData[0])\n",
    "        best_sense, max_overlap, signature_best_sense, common_words_best_sense = lesk(noun, sentence_str,cleanse=cleanse)\n",
    "        if(not best_sense):\n",
    "            if(debug):\n",
    "                print(\"No sense found for the word: \",noun)\n",
    "            best_senses.append(None)\n",
    "        else:\n",
    "            best_senses.append(best_sense)\n",
    "            if(debug):\n",
    "                print(\"Sense for word: \",noun,\n",
    "                '\\n', best_sense,\n",
    "                '\\n', best_sense.definition(),\n",
    "                '\\n', best_sense.examples(),\n",
    "                '\\n', max_overlap,\n",
    "                '\\n', signature_best_sense,\n",
    "                '\\n', common_words_best_sense)\n",
    "    return best_senses\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the accuracy of the implemented Lesk Algo over 50 sample sentences (the first 50).\n",
    "\n",
    "In the first case, we will set the cleanse param to False, so function words and stop words won't be removed.\n",
    "In the second iteration, data cleanse will be turned on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[with stopwords]\n",
      "Correct senses:  115\n",
      "Total senses:  359\n",
      "Accuracy for all sentences:  0.3203342618384401\n",
      "\n",
      "[no stopwords]\n",
      "Correct senses:  77\n",
      "Total senses:  244\n",
      "Accuracy for all sentences:  0.3155737704918033\n"
     ]
    }
   ],
   "source": [
    "def runTest(cleanse=False,randomExtract=False,debug=False):\n",
    "    correct_senses = [0] * 50\n",
    "    total_senses   = [0] * 50\n",
    "    sentences = get_sentences_from_semcor(randomExtract=randomExtract)\n",
    "    for i,sentenceData in enumerate(sentences):\n",
    "        nouns = get_nouns_from_sentence(sentenceData,randomExtract=randomExtract)\n",
    "        best_senses= disambiguate_nouns(sentenceData,nouns,cleanse=cleanse,debug=debug)\n",
    "\n",
    "        if(best_senses):\n",
    "            # https://www.nltk.org/api/nltk.corpus.reader.SemcorCorpusReader.html?highlight=tagged_sent#nltk.corpus.reader.SemcorCorpusReader.tagged_sents\n",
    "            # write the code to check if the sense of the word is the same of the one in semcor\n",
    "\n",
    "            for (nounData,best_sense) in zip(nouns,best_senses):\n",
    "                # check if best_senses[ix].name() exists in sentenceData[1] Lemmas, and if it does, check if the leaf contains the noun\n",
    "                if(best_sense):\n",
    "                    if(best_sense == nounData[1]):\n",
    "                        correct_senses [i] += 1\n",
    "                    total_senses [i] += 1\n",
    "    return (correct_senses, total_senses)\n",
    "print(\"\\n[with stopwords]\")\n",
    "correct_senses, total_senses= runTest(cleanse=False,randomExtract=False,debug=False)\n",
    "debug = False\n",
    "# evaluate the accuracy of the algorithm in respect to each sentence\n",
    "if(debug):\n",
    "    for i in range(50):\n",
    "        if(total_senses[i] > 0):\n",
    "            print(\"Accuracy for sentence \",i,\": \",correct_senses[i]/total_senses[i])\n",
    "        else:\n",
    "            print(\"Accuracy for sentence \",i,\": \",0)\n",
    "# evaluate the accuracy of the algorithm in respect to all the sentences\n",
    "print(\"Correct senses: \",sum(correct_senses))\n",
    "print(\"Total senses: \",sum(total_senses))\n",
    "print(\"Accuracy for all sentences: \",sum(correct_senses)/sum(total_senses))\n",
    "\n",
    "\n",
    "correct_senses, total_senses= runTest(cleanse=True,randomExtract=False,debug=False)\n",
    "print(\"\\n[no stopwords]\")\n",
    "# evaluate the accuracy of the algorithm in respect to each sentence\n",
    "if(debug):\n",
    "    for i in range(50):\n",
    "        if(total_senses[i] > 0):\n",
    "            print(\"Accuracy for sentence \",i,\": \",correct_senses[i]/total_senses[i])\n",
    "        else:\n",
    "            print(\"Accuracy for sentence \",i,\": \",0)\n",
    "# evaluate the accuracy of the algorithm in respect to all the sentences\n",
    "print(\"Correct senses: \",sum(correct_senses))\n",
    "print(\"Total senses: \",sum(total_senses))\n",
    "print(\"Accuracy for all sentences: \",sum(correct_senses)/sum(total_senses))\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Randomize the selection of the 50 sentences and the selection of the term to be \n",
    "disambiguate, and return the average accuracy over (for example) 10 \n",
    "program executions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy  0.162+-0.12 over 10 iterations: \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "correct_senses= []\n",
    "total_senses = []\n",
    "\n",
    "for i in range(10):\n",
    "    correct_senses_i, total_senses_i=  runTest(cleanse=True,randomExtract=True,debug=False)\n",
    "    correct_senses.append(  correct_senses_i)\n",
    "    total_senses.append(total_senses_i)\n",
    "    \n",
    "\n",
    "correct_senses_sum = 0\n",
    "total_senses_sum   = 0\n",
    "accuracies = np.zeros(len(correct_senses))\n",
    "for i in range(len(correct_senses)):\n",
    "    correct_senses_sum += sum(correct_senses[i])\n",
    "    total_senses_sum  += sum(total_senses[i])\n",
    "    # insert a value in the array accuracies\n",
    "    accuracies[i]= sum(correct_senses[i])/sum(total_senses[i])\n",
    "\n",
    "print(\"Average accuracy  {:.3f}+-{:.2f} over {} iterations: \".format(accuracies.mean(),\n",
    "accuracies.std(), len(accuracies)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy  0.303+-0.03 over 10 iterations: \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "correct_senses= []\n",
    "total_senses = []\n",
    "\n",
    "for i in range(10):\n",
    "    correct_senses_i, total_senses_i=  runTest(cleanse=False,randomExtract=True,debug=False)\n",
    "    correct_senses.append(  correct_senses_i)\n",
    "    total_senses.append(total_senses_i)\n",
    "    \n",
    "\n",
    "correct_senses_sum = 0\n",
    "total_senses_sum   = 0\n",
    "accuracies = np.zeros(len(correct_senses))\n",
    "for i in range(len(correct_senses)):\n",
    "    correct_senses_sum += sum(correct_senses[i])\n",
    "    total_senses_sum  += sum(total_senses[i])\n",
    "    # insert a value in the array accuracies\n",
    "    accuracies[i]= sum(correct_senses[i])/sum(total_senses[i])\n",
    "\n",
    "print(\"Average accuracy  {:.3f}+-{:.2f} over {} iterations: \".format(accuracies.mean(),\n",
    "accuracies.std(), len(accuracies)))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summing up\n",
    "The Lesk algorithm is a popular word sense disambiguation algorithm that uses the context of a word to determine its correct meaning. It relies on the overlap of word definitions in a given context to make sense of ambiguous words.\n",
    "Function words are words that have little or no lexical meaning on their own but play a crucial role in the grammatical structure of a sentence. \n",
    "\n",
    "The impact of function words on the evaluation of the Lesk algorithm:\n",
    "- **Contextual Significance**: Function words often carry important contextual information. They help establish relationships between other words in a sentence and provide clues about the syntactic structure. Removing function words may lead to a loss of this contextual information, which can negatively affect the accuracy of the algorithm.\n",
    "- **Sense Discrimination**: Function words are typically highly frequent and appear in various contexts. By including function words in the evaluation, the algorithm can take into account the surrounding function words that may disambiguate the sense of the target word. \n",
    "- **Word Co-occurrence**: Function words often co-occur with content words (such as nouns, verbs, and adjectives) in specific patterns. By considering the function words along with the content words, the Lesk algorithm can leverage the co-occurrence patterns to disambiguate word senses more accurately. Eliminating function words might disrupt these patterns and result in suboptimal disambiguation results.\n",
    "\n",
    "While eliminating function words might reduce noise in some cases, it can also hinder the algorithm's ability to capture the nuances of word sense disambiguation. By retaining function words, the Lesk algorithm can leverage their contributions to achieve better results.\n",
    "**In this case the accouracy improved by around 87%.** (mean over 10 cycles)\n",
    "\n",
    "\n",
    " Function words do not necessarily impact the performance of the Lesk algorithm directly. In fact, their inclusion can often improve the algorithm's results by providing a larger signature and increasing the potential for overlaps."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API TESTING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', 'Atlanta', \"'s\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], [['The'], Tree(Lemma('group.n.01.group'), [Tree('NE', ['Fulton', 'County', 'Grand', 'Jury'])]), Tree(Lemma('state.v.01.say'), ['said']), Tree(Lemma('friday.n.01.Friday'), ['Friday']), ['an'], Tree(Lemma('probe.n.01.investigation'), ['investigation']), ['of'], Tree(Lemma('atlanta.n.01.Atlanta'), ['Atlanta']), [\"'s\"], Tree(Lemma('late.s.03.recent'), ['recent']), Tree(Lemma('primary.n.01.primary_election'), ['primary', 'election']), Tree(Lemma('produce.v.04.produce'), ['produced']), ['``'], ['no'], Tree(Lemma('evidence.n.01.evidence'), ['evidence']), [\"''\"], ['that'], ['any'], Tree(Lemma('abnormality.n.04.irregularity'), ['irregularities']), Tree(Lemma('happen.v.01.take_place'), ['took', 'place']), ['.']])]\n",
      "Lemma('group.n.01.group')\n",
      "(NE Fulton County Grand Jury)\n",
      "Lemma('state.v.01.say')\n",
      "said\n",
      "Lemma('friday.n.01.Friday')\n",
      "Friday\n"
     ]
    }
   ],
   "source": [
    "sentences = get_sentences_from_semcor()\n",
    "\n",
    "for s in sentences[0][1][1:4]:\n",
    "    if isinstance(s, nltk.tree.Tree): \n",
    "        print(s.label())\n",
    "        for word in s:\n",
    "            print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fulton\n",
      "County\n",
      "Grand\n",
      "Jury\n",
      "[None, None, None, None]\n",
      "Lemma('group.n.01.group') Fulton\n",
      "n\n",
      "Lemma('friday.n.01.Friday') Grand\n",
      "n\n",
      "Lemma('probe.n.01.investigation') said\n",
      "n\n",
      "Lemma('atlanta.n.01.Atlanta') an\n",
      "n\n",
      "Lemma('primary.n.01.primary_election') Atlanta\n",
      "n\n",
      "Lemma('evidence.n.01.evidence') election\n",
      "n\n",
      "Lemma('abnormality.n.04.irregularity') evidence\n",
      "n\n",
      "['Fulton', 'Grand', 'said', 'an', 'Atlanta', 'election', 'evidence']\n",
      "group.n.01\n",
      "\n",
      "\n",
      "j\n",
      "u\n",
      "r\n",
      "y\n",
      "[None, None, None, None]\n",
      "Lemma('jury.n.01.jury') jury\n",
      "n\n",
      "Lemma('term.n.02.term') term\n",
      "n\n",
      "Lemma('end.n.02.end') end\n",
      "n\n",
      "Lemma('presentment.n.01.presentment') presentments\n",
      "n\n",
      "Lemma('group.n.01.group') City\n",
      "n\n",
      "Lemma('mission.n.03.charge') had\n",
      "n\n",
      "Lemma('election.n.01.election') of\n",
      "n\n",
      "Lemma('praise.n.01.praise') deserves\n",
      "n\n",
      "Lemma('thanks.n.01.thanks') praise\n",
      "n\n",
      "Lemma('location.n.01.location') of\n",
      "n\n",
      "Lemma('manner.n.01.manner') Atlanta\n",
      "n\n",
      "Lemma('election.n.01.election') manner\n",
      "n\n",
      "['jury', 'term', 'end', 'presentments', 'City', 'had', 'of', 'deserves', 'praise', 'of', 'Atlanta', 'manner']\n",
      "jury.n.01\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# retrieve sentence sense from semcor\n",
    "def get_sentence_sense_from_semcor(sentence):\n",
    "    for i in range(len(semcor.tagged_sents())):\n",
    "        if(semcor.sents()[i] == sentence):\n",
    "            return semcor.tagged_sents()[i]\n",
    "\n",
    "\n",
    "\n",
    "def get_nouns_from_sentence(sentenceData):\n",
    "    nouns = []\n",
    "\n",
    "    for pos, targetSense in enumerate(sentenceData[1]):\n",
    "        if isinstance(targetSense, nltk.tree.Tree):\n",
    "            lemma = targetSense.label()\n",
    "            if(lemma):\n",
    "                synset = None\n",
    "                if(isinstance(lemma,str)):\n",
    "                   synset = wn.synsets(lemma)\n",
    "                else:\n",
    "                    synset = lemma.synset()\n",
    "                if synset and isinstance(synset,Synset) and  synset.pos() == 'n':\n",
    "                    print(lemma,sentenceData[0][pos])\n",
    "                    # print(synset)\n",
    "                    print(synset.pos())\n",
    "                    nouns.append(sentenceData[0][pos])\n",
    "            \n",
    "    return nouns\n",
    "\n",
    "      \n",
    "for i in zip(semcor.tagged_sents(tag=\"sem\")[:2],semcor.sents()[:2]):\n",
    "    print( [print(x) for x in i[0][1][0]])\n",
    "    print(get_nouns_from_sentence((i[1],i[0])))\n",
    "    # access the lemma of the word\n",
    "    # get the type of i[0][1]\n",
    "    # it is a nltk.tree.tree.Tree, so how to access the lemma?\n",
    "    print(i[0][1].label().synset().name())\n",
    "    # print(i[0][1].label().synset())\n",
    "    \n",
    "    print(\"\\n\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
